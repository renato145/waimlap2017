{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = np.random.choice(60000)\n",
    "plt.imshow(x_train[random_idx], 'gray')\n",
    "plt.title(y_train[random_idx])\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar las imagenes\n",
    "x_train = (x_train - 127.5) / 127.5\n",
    "x_test = (x_test - 127.5) / 127.5\n",
    "\n",
    "# Redimensionar las imagenes\n",
    "# TODO\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ojo: Esta vez no vamos a formatear nuestros labels a modo one-hot encode.\n",
    "\n",
    "Si usamos la función de perdida \"sparse_categorical_crossentropy\", el modelo tratará los labels como si fueran one-hot encode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construyendo el modelo en Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# definir el modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir manualmente un batch de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Obtener un minibatch\n",
    "# TODO:\n",
    "batch_x.shape, batch_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Entrenar el batch\n",
    "loss = model.train_on_batch(batch_x, batch_y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actualizar el learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el learning rate del optimizador de nuestro modelo\n",
    "K.eval(model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar nuevo valor de learning rate\n",
    "K.eval(model.optimizer.lr.assign(0.002))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encontrar un buen learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Definir una lista de learning rates a probar\n",
    "iterations = 500\n",
    "lr_list = 10 ** np.linspace(-4, 0, (iterations+1))\n",
    "print(lr_list[0], '->', lr_list[-1])\n",
    "plt.plot(lr_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define una funcion que realize un batch de entrenamiento,\n",
    "#    con un learning rate específico y retorne la perdida.\n",
    "\n",
    "def get_loss(lr):\n",
    "    # TODO\n",
    "    \n",
    "    return loss\n",
    "\n",
    "get_loss(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Obtengamos la perdida por cada learning rate en lr_list.\n",
    "#    Antes de ejecutar este paso definan nuevamente el modelo,\n",
    "#    para resetear los pesos.\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i, lr in enumerate(lr_list):\n",
    "    print('%d/%d...' % (i+1, len(lr_list)), end='\\r')\n",
    "    losses.append(get_loss(lr))\n",
    "    if losses[-1] > np.min(losses)*6:\n",
    "        # Si la perdida se hace muy grande, detenemos el loop\n",
    "        break\n",
    "\n",
    "print('\\nDone :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Observemos la perdida vs el learning rate\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(lr_list[:len(losses)], losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('learning rate (log scale)')\n",
    "plt.xscale('log')\n",
    "plt.ylim(0,4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definamos nuevamente el modelo y usemos el lr encontrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning decay\n",
    "\n",
    "El optimizador tiene un parametro decay:\n",
    "* Asignenle un valor.\n",
    "* Definan un loop de entrenamiento.\n",
    "* Visualizen el cambio de learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
